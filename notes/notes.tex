\documentclass[a4paper, 11pt]{article}

\usepackage[top=112pt, bottom=112pt, left=90pt, right=85pt]{geometry}
\usepackage{geometry, amssymb, csquotes, amsmath, graphicx, mathtools, amsthm, calc, accents} 
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{todonotes}

\newcommand{\rem}[2][noinline]{\todo[#1, color=gray!20!white,size=\footnotesize]{\texttt{Rem}: #2}}
\newcommand{\doubletilde}[1]{\tilde{\raisebox{0pt}[0.85\height]{$\tilde{#1}$}}}
\newcommand{\tripletilde}[1]{\tilde{\raisebox{0pt}[0.85\height]{$\doubletilde{#1}$}}}

\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theor}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{conjecture}{Conjecture}

\usepackage{tcolorbox}
\usepackage{lipsum} % for dummy text

% Define a custom style for the question box
\tcbset{
  myquestionbox/.style={
    colback=cyan!10!white,
    colframe=cyan!50!black,
    coltitle=black,
    fonttitle=\bfseries,
    title=Something to try/think about,
    boxrule=0.5mm,
    sharp corners,
    enhanced,
    width=\textwidth
  }
}

\DeclareMathOperator\artanh{artanh}
\DeclareMathOperator\tr{tr}

\hypersetup{
  urlcolor = black,
  citecolor = black,
  pdftitle = {notes},
  pdfsubject = {notes},
  pdfpagemode = UseNone
}

\newcommand\underl[2]{\mathrel{\mathop{#2}\limits_{#1}}}

\title{\textbf{Neuromatch NeuroAI Microlearning project}}
\date{\today}
\author{NevroA6 crew}
\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\tableofcontents

\section{Single-layer Hebbian network vs. softmax classifier}
In this section we compare the performances and the weight matrices of the single-layer models trained by {\it Hebbian learning} and the {\it stochastic gradient descent} (SGD) on the subsets of MNIST hand written digits dataset.
\subsection{Hebbian learning}
In the Hebbian learning setting the weights are set as follows
\begin{equation} \label{Hebbian_weights}
  \mathbf W = \eta\sum_{\alpha=1}^{N_d} \mathbf t^{(\alpha)} \otimes \mathbf x^{(\alpha)},
\end{equation}
where $\mathbf x^{(\alpha)}$ is the $28\times 28=784$-dimensional input at datapoint $\alpha$, $\mathbf t^{(\alpha)}$ is the corresponding one-hot encoded target output and $\otimes$ is stands for the outer product defined as $(\mathbf a\otimes \mathbf b)_{ij} = a_ib_j$, ${N_d}$ is the number of training samples and $\eta$ is the learning rate (which has no effect on the predictions in this setting).

The activation received at the output layer computed as $\mathbf y = \mathbf W\mathbf x$ is thus given by
\begin{equation} \label{1-layer_Hebbian_output}
  \mathbf y = \eta \sum_{\alpha=1}^{N_d}\mathbf t^{(\alpha)}(\mathbf x^{(\alpha)},\mathbf x),
\end{equation}
where $\mathbf x$ is the input and $(\mathbf x^{(\alpha)},\mathbf x)$ is the dot product of the current input $\mathbf x$ with the input at the datapoint $\alpha$. The above expression (\ref{1-layer_Hebbian_output}) is a weighted average of the target outputs, where the weights are proportional to the cosine similarity between the current input and the corresponding input in the data.

Note that a similar expression to expression (\ref{1-layer_Hebbian_output}) is valid when the network is reversed, i.e. $\mathbf x = \mathbf W^T\mathbf y$
\begin{equation}
  \mathbf x = \eta\sum_{\alpha=1}^{N_d}\mathbf x^{(\alpha)}(\mathbf t^{(\alpha)},\mathbf y).
\end{equation}
The images resulting from such {\it reverse inference}\footnote{\textcolor{red}{I don't know the proper name for this.}} is shown in Fig.\ref{fig:reverse_inference}, and it is easy to see that the inputs generated this way are simply the averaged inputs for the corresponding classes.

\begin{figure}
  \begin{center}
    \includegraphics[width=16cm]{img/flattened_weight_matrices.pdf}
  \end{center}  
  \caption{Vertical stack of the different rows of this image reproduces the reverse activations from Fig.\ref{fig:reverse_inference}.}
  \label{fig:flattened_weight_matrices}
\end{figure}


\begin{figure}
  \begin{center}
    \includegraphics[width=16cm]{img/reverse_inference.pdf}
  \end{center}  
  \caption{The results form running the network in reverse, i.e. $\mathbf x = \mathbf W^T\mathbf y$ from the one-hot encoded target class labels for the networks trained by Hebbian (top panels) learning and SGD (bottom panels).}
  \label{fig:reverse_inference}
\end{figure}

To find the predicted probabilities of different classes, one can apply softmax to the output, but to find the predicted label it is enough to pick the output unit with the highest activation (as the corresponding class will have the highest predicted probability). Typical accuracies of the network used this way are shown in Fig.\ref{accuracies_of_Hebbian_model}, where accuracy is defined as
\begin{equation}
  (\text{Accuracy}) = 1 - \frac{(\text{Number of misclassified inputs})}{(\text{Total number of data points})}.
\end{equation}

\begin{figure}
    \centering
    \begin{minipage}{.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/Hebbian_training_results_1.png}
        % \caption{}
    \end{minipage}\hfill
    \begin{minipage}{.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/Hebbian_training_results_2.png}
        % \caption{}
    \end{minipage}\hfill
    \begin{minipage}{.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/Hebbian_training_results_3.png}
        % \caption{}
    \end{minipage}
    \caption{Performances of the Hebbian network on different subsets of MNIST hand written digits dataset.}
    \label{accuracies_of_Hebbian_model}
\end{figure}

Note that expression (\ref{Hebbian_weights}) can be regarded as a single-epoch online learning (learning with batch size of one) where the output layer is clamped to the one-hot encoded targets. In this setting there is no need to train the network for more than one epoch, as this does not affect the performance.

\textcolor{blue}{It is interesting what kind of loss function is minimised by this Hebbian learning procedure...}

\subsection{Softmax classifier}
The output layer now computes the softmax function defined as
\begin{equation} \label{softmax_definition}
  \sigma_i = \frac{\mathrm e^{l_i}}{\sum_{k=1}^{N_c}\mathrm e^{l_k}}, 
\end{equation}
where the logits $l_i$ are given as inputs to the output layer $\mathbf l = \mathbf W \mathbf x$, and $N_c$ is the number of different classes ($10$ in this case). Substituting expression \ref{softmax_definition} to the cross-entropy loss
\begin{equation}
  \mathcal L(\boldsymbol\sigma, \mathbf t) = -\frac{1}{{N_d}}\sum_{\alpha=1}^{N_d}\mathbf t^{(\alpha)}\log\boldsymbol\sigma^{(\alpha)}
\end{equation}
gives
\begin{equation*}
  \mathcal L(\boldsymbol\sigma, \mathbf t) = -\frac{1}{{N_d}}\sum_{\alpha=1}^{N_d}\mathbf t^{(\alpha)}\left[l_i - \log\left(\sum_{k=1}^{N_c}\mathrm e^{l_k}\right)\right],
\end{equation*}
whose partial derivatives with respect to the weights are given by
\begin{equation}
  \frac{\partial \mathcal L}{\partial W_{ij}} = \frac{1}{{N_d}}\sum_{\alpha=1}^{N_d}\left(t_i^{(\alpha)} - \sigma_i^{(\alpha)}\right)x_j^{(\alpha)},
\end{equation}
or in a vector form
\begin{equation}\label{cross-entropy_derivative}
  \frac{\partial \mathcal L}{\partial \mathbf W} = \frac{1}{{N_d}}\sum_{\alpha=1}^{N_d}\left(\mathbf t^{(\alpha)} - \boldsymbol\sigma^{(\alpha)}\right)\otimes\mathbf x^{(\alpha)}.
\end{equation}
From expression (\ref{cross-entropy_derivative}) it is clear that the weight updates obtained using gradient descent (without batching) are given by
\begin{equation}
  \Delta\mathbf W = \eta\frac{\partial \mathcal L}{\partial \mathbf W} = \frac{\eta}{{N_d}}\sum_{\alpha=1}^{N_d}\mathbf t^{(\alpha)}\otimes\mathbf x^{(\alpha)} - \frac{\eta}{{N_d}}\sum_{\alpha=1}^{N_d}\boldsymbol\sigma^{(\alpha)}\otimes\mathbf x^{(\alpha)},
\end{equation}
where the first term in the above expression corresponds to the Hebbian component of the weight update while the second introduces some bias that grows with the network's performance compensating Hebbian component once the optimal performance is reached. In contrast, in pure Hebbian learning, the weights grow indefinitely, \textcolor{red}{therefore, the bias of Hebbian learning depends on how it is regularised.}

\begin{tcolorbox}[myquestionbox]
Is it possible to improve the performance of Hebbian network by embedding the labels into higher-dimensional space (instead of 10 dimensions with one-hot encoding) to increase the number of tunable parameters?
\end{tcolorbox}

\section{Why hidden layers decrease Hebbian model performance}

While adding the hidden layers increases the number of tunable parameters, Hidden layers obscure the input layer from the output layer in the following sense. Since the weights of the hidden layer are initialised randomly and the hidden layer's activity does not have any task-relevant loss function (it does not receive anything from the upstream layers), the dynamics of the weights under Hebbian learning is completely determined by the random initialisation of the input-to-hidden weight matrix. (This explains the variability in performance we observe from trial to trial.) The output layer then tries to extract the structure from the activity of the hidden layer, but this is harder than looking directly at the input because the hidden layer didn't have any task-relevant objective function and thus didn't learn anything useful.

\section{Forward-Forward algorithm}
Forward-Forward (FF) algorithm is a contrastive learning algorithm in which every layer greedily (without caring about the performance of other layers) learns to predict whether the data is positive (real) or negative (unreal). One way of doing that is to set the class probability estimates as
\begin{align*}
  P_+(\prescript{}{l}{\mathbf y}) &= \sigma(\prescript{}{l}{\mathbf y}^2-\theta); \\
  P_-(\prescript{}{l}{\mathbf y}) &= 1-P_+(\prescript{}{l}{\mathbf y}),
\end{align*}
$P_+$ and $P_-$ are the predicted probabilities of positive and negative data respectively, $\prescript{}{l}{\mathbf y}^2 \equiv \sum_{i=1}^{N_l}\prescript{}{l}{y}_i^2$ is the squared length of the layers activity, and $\sigma(z) \equiv \frac{\mathrm e^z}{1+\mathrm e^z}$ is a sigmoid function. To achieve high performance on this task every layer needs to learn to have high activity vector length for positive data and low activity vector length for negative data, thus the activity vector length can be regarded as a {\bf local} goodness function optimised by the layer at training.

Since the goodness function is local to all layers, there is no need to propagate the error signal through the network backwards. Instead, every layer locally computes the error and updates its input weights on every training step, which is more biologically plausible and easier to implement in hardware.

The only caveat, which is not clearly biologically plausible, is that, in order to prevent information about the goodness function of the upstream layers to go to the downstream layers and making it very easy for them to learn how to optimise their goodness\footnote{Without normalisation in the upstream layers, the downstream layers can learn to directly measure goodness of the upstream layers and repeat it. By doing so, they can achieve high performance without learning useful representations of the data. Since the goodness function is the length of the activity vector of the layer, $L^2$-normalisation entirely removes goodness information and the downstream layers are forced to extract useful representations from the relative activities of the units projecting to them.}

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=7cm]{img/FF_net.pdf}
  \end{center}  
  \caption{In FF algorithm every layer learns to predict independently (in some sense) whether the data is positive (real) or negative (unreal). Vertical rectangles indicate normalisation.}
\end{figure}

\subsection{FF algorithm and Hebbian/anti-Hebbian learning}
In FF algorithm the goodnesses of all layers are optimised using stochastic gradient descent, but it turns out that the weight updates resulting from SGD are Hebbian for positive data and anti-Hebbian for negative data.

To see this, consider a step of gradient descent in the layer $l+1$ from a single data sample
\begin{align*}
  \frac{\partial}{\partial\prescript{}{l+1}W_{\alpha\beta}}\sum_i \left(f\left(\sum_j\prescript{}{l+1}W_{ij}\prescript{}{l}{\widetilde{y}}_j\right)\right)^2 = 2f^\prime\left(\sum_j\prescript{}{l+1}W_{\alpha j}\prescript{}{l}{\widetilde{y}}_j\right)\prescript{}{l+1}{y}_\alpha\prescript{}{l}{\widetilde{y}}_\beta,
  \end{align*}
  where $f$ is the layers activation function (ReLU in \cite{hinton2022forwardforwardalgorithmpreliminaryinvestigations}), and tilda over the activity variable denotes normalisation (${\widetilde{y}}_j = y_j/\|\mathbf y\|,\ \ \|\mathbf y\| = \sqrt{\mathbf y^2}$). So for positive data
  \begin{equation*}
    \Delta \prescript{}{l+1}W_{\alpha\beta} = 2\epsilon f^\prime\left(\sum_j\prescript{}{l+1}W_{\alpha j}\prescript{}{l}{\widetilde{y}}_j\right)\prescript{}{l+1}{y}_\alpha\prescript{}{l}{\widetilde{y}}_\beta,
  \end{equation*}
  and for negative data
  \begin{equation*}
    \Delta \prescript{}{l+1}W_{\alpha\beta} = -2\epsilon f^\prime\left(\sum_j\prescript{}{l+1}W_{\alpha j}\prescript{}{l}{\widetilde{y}}_j\right)\prescript{}{l+1}{y}_\alpha\prescript{}{l}{\widetilde{y}}_\beta,
  \end{equation*}
  where $\epsilon$ is the learning rate.



\bibliographystyle{unsrt}
\bibliography{bibliography.bib}

\end{document}
